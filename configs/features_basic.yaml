objectives:
  lambda_importance: 1.0
  lambda_coverage: 0.8
  lambda_redundancy: 0.7
  length_penalty: 2.0
representations:
  use: true
  method: "tfidf"  # or "sbert"
  cache: true
candidates:
  use: true
  k: 15
  mode: "hard"        # hard | soft
  sources: ["score"]  # union of: score | position | centrality
  soft_boost: 1.05     # only used when mode=soft (multiplicative boost for candidates)
  recall_target: null  # e.g., 0.95 (optional; requires reference to measure oracle recall)
redundancy:
  method: "mmr"    # or "jaccard"
  lambda: 0.7
  sim_metric: "cosine"
length_control:
  unit: "tokens"   # tokens | sentences
  max_tokens: 100
optimizer:
  method: "greedy" # greedy | grasp | nsga2 | bart | pegasus
seed: 2024

# 外部化特徵權重（未設定時會退回預設 importance=1.0, length=0.3, position=0.3）
features:
  weights:
    importance: 1.0
    length: 0.3
    position: 0.3

rerank:
  enabled: false
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_n: 20
  normalize: "minmax"   # minmax | zscore
  weights:               # fusion weights (per-document normalized)
    ce: 1.0
    base: 0.0
# 基本設定：單階段（以 tokens 控制長度），TF‑IDF 表示 + Greedy。

objectives:
  lambda_importance: 1.0
  lambda_coverage: 0.8
  lambda_redundancy: 0.7
  length_penalty: 2.0   # 目前未直接使用（保留）

features:
  # 可選：外部化特徵權重；未提供時仍會有預設
  weights:
    importance: 1.0
    length: 0.3
    position: 0.3

representations:
  use: true             # 若為 false，將跳過向量/相似度
  method: tfidf         # tfidf | sbert（需 sentence-transformers）
  cache: false

candidates:
  use: true
  k: 15
  mode: hard            # hard | soft
  sources: [score]      # score | position | centrality
  soft_boost: 1.05
  recall_target: null   # 例如 0.95；validation 調參時可用

redundancy:
  method: mmr
  lambda: 0.7           # MMR 調和係數
  sim_metric: cosine

length_control:
  unit: tokens          # tokens | sentences
  max_tokens: 100

optimizer:
  method: greedy        # greedy | grasp | nsga2 | bert | fused | fast

seed: 2024
