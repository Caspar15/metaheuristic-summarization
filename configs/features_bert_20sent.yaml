objectives:
  lambda_importance: 1.0
  lambda_coverage: 0.8
  lambda_redundancy: 0.7
  length_penalty: 2.0
representations:
  use: false          # 讓 BERT 排序不受候選池/相似度影響，對全文取 K2
candidates:
  use: false          # 第二層 K2 需對全文排序，關閉候選池
redundancy:
  method: "mmr"
  lambda: 0.7
  sim_metric: "cosine"
length_control:
  unit: "sentences"
  max_tokens: 100
  max_sentences: 20
optimizer:
  method: "bert"
seed: 2024

bert:
  model_name: "google-bert/bert-base-uncased"
# BERT 排序（不訓練），取 20 句（Stage1 K2）

length_control:
  unit: sentences
  max_sentences: 20

optimizer:
  method: bert

bert:
  model_name: bert-base-uncased

# Stage1 K2 僅用 BERT 排序，不需向量/相似度
representations:
  use: false

seed: 2024
